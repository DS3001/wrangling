{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13ad028b-72b7-43ed-aa78-96fd4e518040",
   "metadata": {},
   "source": [
    "# Assignment: Data Wrangling "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5735a4d4-8be8-433a-a351-70eb8002e632",
   "metadata": {},
   "source": [
    "**Q1.** Open the \"tidy_data.pdf\" document in the folder, which is a paper called Tidy Data by Hadley Wickham.\n",
    "\n",
    "    1. Read the abstract. What is this paper about?\n",
    "\n",
    "> Wickham is interested in thinking more abstractly about the process of data cleaning, which probably doesn't receive as much attention as it should. Removing `NA`'s is great, but beyond that, what should data look like in order to decide you're done cleaning? He introduces a criteria (each row is an observation, each column is a variable, each type of observational unit is a table) and explore the consequences.\n",
    "\n",
    "  2. Read the introduction. What is the \"tidy data standard\" intended to accomplish? \n",
    "  \n",
    "> Despite being a large part of data analysis --- both in terms of the time it requires and the coneceptual effort invested in it --- data cleaning is understudied as a skill or activity. The \"tidy data standard\" is meant to standardize data cleaning. The goal of the standard is to make it easier to clean data, since everyone involved knows what the objective is and the usual steps required to succeed. \n",
    "  \n",
    "  3. Read the intro to section 2. What does this sentence mean: \"Like families, tidy datasets are all alike but every messy dataset is messy in its own way.\" What does this sentence mean: \"For a given dataset, itâ€™s usually easy to figure out what are observations and what are variables, but it is surprisingly difficult to precisely define variables and observations in general.\"\n",
    "  \n",
    "> The idea in the first sentence is that dysfunctional and messy data often present unique or novel problems, while clean data all have similar properties. You have probably experienced this already: Whoever created the data structured it for their purposes or out of convenience, and manipulating it successful within R requires substantial effort. This is a Russian literature joke. The second sentence acknowledges that the idea of a \"Data frame/matrix\" is intuitive -- a row is an observation, a column is a variable -- but in practice, this is a choice that often benefits the analyst if it is made consciously. For example, if you have data for counties by year, what \"are\" your data? An observation is a county-year, which might not be obvious to someone who hasn't played with that kind of data before, who might conclude that an observation is a county; this would lead to a lot of unintentionally bad choices about cleaning and organizing the data.\n",
    "  \n",
    "  4. Read Section 2.2. How does Wickham define values, variables, and observations?\n",
    "  \n",
    "  > A dataset is a collection of values. Values are numeric or categorical/strings. Every value belongs to both a variable and an observation. A variable is a collection of values that measure the same attribute or property (e.g. height, color, temperature, make/model).  An observation is a collection of values that measure it.\n",
    "  \n",
    "  5. How is \"Tidy Data\" defined in section 2.3?\n",
    "  \n",
    "> In tidy data, each variable is a column, each observation is a row, and each type of observational unit is a table. If data is not tidy, it is messy. \n",
    "  \n",
    "  6. Read the intro to Section 3 and Section 3.1. What are the 5 most common problems with messy datasets? Why are the data in Table 4 messy? What is \"melting\" a dataset?\n",
    "  \n",
    "> 1. Column headers are values (like year), and not variable names. So for example, unemployment by county over time is represented as a matrix where columns are years and rows are counties. The column header is really a numeric like 2012, which is a value, not a variable name, like \"Year\". 2. Multiple variables are stored in one column. For example, month-day-year dates include three things: month, day, and year. Time variables inevitably have to be converted into another kind of format (e.g. normalized to a \"0\" date, and then time measured in numbers of days since day 0). 3. Variables are stored in both rows and columns. 4. Multiple types of observational units are stored in the same table. For example, entrepreneurs/firms, children/parents, workers/firms all get lumped into one big file rather than separate datasets for the two groups. 5. A single observational unit is stored in multiple tables. Data get repeated in an inconvenient way, required subsequent simplification and cleaning. In Table 4, the columns are really values of a \"hidden\" variable, which is income. Since income is actually a variable, you need a new column, `income`, alongside `religion`, and then frequency, as in table 6. Now, the columns are all the names of variables, rather than the values that variables take. Melting a dataset is this process of converting column-value variables into rows.\n",
    "  \n",
    "  7. Why, specifically, is table 11 messy but table 12 tidy and \"molten\"?\n",
    "  \n",
    "  > Table 11 has days along the top, which are values. Table 12 melts those days into a single variable, `date`. That still isn't tidy, because the `element` variable contains variable names and not values --- `tmax` and `tmin` are measurements of the same day, which are really names of variables and not values themselves. Table 12(b) is tidy because all the entries are attributes and not variable names.\n",
    "  \n",
    "  8. Read Section 6. What is the \"chicken-and-egg\" problem with focusing on tidy data? What does Wickham hope happens in the future with further work on the subject of data wrangling?\n",
    "\n",
    "> What Wickham wants is a broader philosophy of data cleaning. If the tidy framework is just about facilitating certain tools, it's just marketing. Wickham is hoping that the tidy concept isn't just about training people to use ggplot2 effectively, but in creating a bigger and more robust ecosystem of ideas and tools for data science in general.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a04690b-b754-47f1-8278-a9927a1ba604",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da879ea7-8aac-48a3-b6c2-daea56d2e072",
   "metadata": {},
   "source": [
    "**Q2.** This question provides some practice cleaning variables which have common problems.\n",
    "1. Numeric variable: For `./data/airbnb_hw.csv`, clean the `Price` variable as well as you can, and explain the choices you make. How many missing values do you end up with? (Hint: What happens to the formatting when a price goes over 999 dollars, say from 675 to 1,112?)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8aacc4c6-0eef-4fbd-afdc-35e1fb0c669a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30478, 13) \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Host Id</th>\n",
       "      <th>Host Since</th>\n",
       "      <th>Name</th>\n",
       "      <th>Neighbourhood</th>\n",
       "      <th>Property Type</th>\n",
       "      <th>Review Scores Rating (bin)</th>\n",
       "      <th>Room Type</th>\n",
       "      <th>Zipcode</th>\n",
       "      <th>Beds</th>\n",
       "      <th>Number of Records</th>\n",
       "      <th>Number Of Reviews</th>\n",
       "      <th>Price</th>\n",
       "      <th>Review Scores Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5162530</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1 Bedroom in Prime Williamsburg</td>\n",
       "      <td>Brooklyn</td>\n",
       "      <td>Apartment</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Entire home/apt</td>\n",
       "      <td>11249.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>145</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>33134899</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sunny, Private room in Bushwick</td>\n",
       "      <td>Brooklyn</td>\n",
       "      <td>Apartment</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Private room</td>\n",
       "      <td>11206.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>39608626</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sunny Room in Harlem</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>Apartment</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Private room</td>\n",
       "      <td>10032.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>500</td>\n",
       "      <td>6/26/2008</td>\n",
       "      <td>Gorgeous 1 BR with Private Balcony</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>Apartment</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Entire home/apt</td>\n",
       "      <td>10024.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>199</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>500</td>\n",
       "      <td>6/26/2008</td>\n",
       "      <td>Trendy Times Square Loft</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>Apartment</td>\n",
       "      <td>95.0</td>\n",
       "      <td>Private room</td>\n",
       "      <td>10036.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>39</td>\n",
       "      <td>549</td>\n",
       "      <td>96.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Host Id Host Since                                Name Neighbourhood   \\\n",
       "0   5162530        NaN     1 Bedroom in Prime Williamsburg       Brooklyn   \n",
       "1  33134899        NaN     Sunny, Private room in Bushwick       Brooklyn   \n",
       "2  39608626        NaN                Sunny Room in Harlem      Manhattan   \n",
       "3       500  6/26/2008  Gorgeous 1 BR with Private Balcony      Manhattan   \n",
       "4       500  6/26/2008            Trendy Times Square Loft      Manhattan   \n",
       "\n",
       "  Property Type  Review Scores Rating (bin)        Room Type  Zipcode  Beds  \\\n",
       "0     Apartment                         NaN  Entire home/apt  11249.0   1.0   \n",
       "1     Apartment                         NaN     Private room  11206.0   1.0   \n",
       "2     Apartment                         NaN     Private room  10032.0   1.0   \n",
       "3     Apartment                         NaN  Entire home/apt  10024.0   3.0   \n",
       "4     Apartment                        95.0     Private room  10036.0   3.0   \n",
       "\n",
       "   Number of Records  Number Of Reviews Price  Review Scores Rating  \n",
       "0                  1                  0   145                   NaN  \n",
       "1                  1                  1    37                   NaN  \n",
       "2                  1                  1    28                   NaN  \n",
       "3                  1                  0   199                   NaN  \n",
       "4                  1                 39   549                  96.0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./data/airbnb_hw.csv', low_memory=False)\n",
    "print( df.shape, '\\n')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44cc53fb-a089-4cd6-b257-982e52191a11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['145', '37', '28', '199', '549', '149', '250', '90', '270', '290',\n",
       "       '170', '59', '49', '68', '285', '75', '100', '150', '700', '125',\n",
       "       '175', '40', '89', '95', '99', '499', '120', '79', '110', '180',\n",
       "       '143', '230', '350', '135', '85', '60', '70', '55', '44', '200',\n",
       "       '165', '115', '74', '84', '129', '50', '185', '80', '190', '140',\n",
       "       '45', '65', '225', '600', '109', '1,990', '73', '240', '72', '105',\n",
       "       '155', '160', '42', '132', '117', '295', '280', '159', '107', '69',\n",
       "       '239', '220', '399', '130', '375', '585', '275', '139', '260',\n",
       "       '35', '133', '300', '289', '179', '98', '195', '29', '27', '39',\n",
       "       '249', '192', '142', '169', '1,000', '131', '138', '113', '122',\n",
       "       '329', '101', '475', '238', '272', '308', '126', '235', '315',\n",
       "       '248', '128', '56', '207', '450', '215', '210', '385', '445',\n",
       "       '136', '247', '118', '77', '76', '92', '198', '205', '299', '222',\n",
       "       '245', '104', '153', '349', '114', '320', '292', '226', '420',\n",
       "       '500', '325', '307', '78', '265', '108', '123', '189', '32', '58',\n",
       "       '86', '219', '800', '335', '63', '229', '425', '67', '87', '1,200',\n",
       "       '158', '650', '234', '310', '695', '400', '166', '119', '62',\n",
       "       '168', '340', '479', '43', '395', '144', '52', '47', '529', '187',\n",
       "       '209', '233', '82', '269', '163', '172', '305', '156', '550',\n",
       "       '435', '137', '124', '48', '279', '330', '5,000', '134', '378',\n",
       "       '97', '277', '64', '193', '147', '186', '264', '30', '3,000',\n",
       "       '112', '94', '379', '57', '415', '236', '410', '214', '88', '66',\n",
       "       '71', '171', '157', '545', '1,500', '83', '96', '1,800', '81',\n",
       "       '188', '380', '255', '505', '54', '33', '174', '93', '740', '640',\n",
       "       '1,300', '440', '599', '357', '1,239', '495', '127', '5,999',\n",
       "       '178', '348', '152', '242', '183', '253', '750', '259', '365',\n",
       "       '273', '197', '397', '103', '389', '355', '559', '38', '203',\n",
       "       '999', '141', '162', '333', '698', '46', '360', '895', '10', '41',\n",
       "       '206', '281', '449', '388', '212', '102', '201', '2,750', '4,750',\n",
       "       '432', '675', '167', '390', '298', '339', '194', '302', '211',\n",
       "       '595', '191', '53', '361', '480', '8,000', '4,500', '459', '997',\n",
       "       '345', '216', '218', '111', '735', '276', '91', '490', '850',\n",
       "       '398', '36', '775', '267', '625', '336', '2,500', '176', '725',\n",
       "       '3,750', '469', '106', '460', '287', '575', '227', '263', '25',\n",
       "       '228', '208', '177', '880', '148', '116', '685', '470', '217',\n",
       "       '164', '61', '645', '699', '405', '252', '319', '268', '419',\n",
       "       '343', '525', '311', '840', '154', '294', '950', '409', '184',\n",
       "       '257', '204', '241', '2,000', '412', '121', '288', '196', '900',\n",
       "       '647', '524', '1,750', '309', '510', '1,495', '1,700', '799',\n",
       "       '383', '372', '492', '327', '1,999', '656', '224', '173', '875',\n",
       "       '1,170', '795', '690', '146', '465', '1,100', '151', '274', '429',\n",
       "       '825', '282', '256', '1,111', '620', '271', '161', '51', '855',\n",
       "       '579', '1,174', '430', '20', '899', '649', '485', '181', '455',\n",
       "       '4,000', '243', '342', '590', '560', '374', '437', '232', '359',\n",
       "       '985', '31', '244', '254', '723', '237', '428', '370', '34',\n",
       "       '1,400', '580', '2,520', '221', '749', '1,600', '2,695', '306',\n",
       "       '202', '680', '570', '520', '223', '2,295', '213', '1,065', '346',\n",
       "       '24', '286', '296', '266', '26', '995', '1,368', '393', '182',\n",
       "       '635', '258', '780', '589', '347', '1,250', '1,350', '446',\n",
       "       '3,200', '1,050', '1,650', '1,550', '975', '323', '6,500', '2,499',\n",
       "       '1,850', '2,250', '715', '461', '540', '356', '439', '384', '569',\n",
       "       '1,900', '22', '785', '626', '830', '318', '444', '321', '401',\n",
       "       '1,499', '888', '369', '770', '386', '366', '344', '630', '313',\n",
       "       '597', '262', '509', '10,000', '278', '312', '789', '1,195', '422',\n",
       "       '21', '765', '3,500', '945', '326', '3,100', '2,486', '3,390',\n",
       "       '1,356', '2,599', '472', '454', '328', '396', '291'], dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "price = df['Price']\n",
    "price.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7806d327-2b6e-4d85-89f8-6abcfad3f815",
   "metadata": {},
   "source": [
    "> It was imported as a string, because there's a comma separator for thousands that we need to eliminate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f053b2ec-42c0-4a5f-9410-1c20404380e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['145' '37' '28' '199' '549' '149' '250' '90' '270' '290' '170' '59' '49'\n",
      " '68' '285' '75' '100' '150' '700' '125' '175' '40' '89' '95' '99' '499'\n",
      " '120' '79' '110' '180' '143' '230' '350' '135' '85' '60' '70' '55' '44'\n",
      " '200' '165' '115' '74' '84' '129' '50' '185' '80' '190' '140' '45' '65'\n",
      " '225' '600' '109' '1990' '73' '240' '72' '105' '155' '160' '42' '132'\n",
      " '117' '295' '280' '159' '107' '69' '239' '220' '399' '130' '375' '585'\n",
      " '275' '139' '260' '35' '133' '300' '289' '179' '98' '195' '29' '27' '39'\n",
      " '249' '192' '142' '169' '1000' '131' '138' '113' '122' '329' '101' '475'\n",
      " '238' '272' '308' '126' '235' '315' '248' '128' '56' '207' '450' '215'\n",
      " '210' '385' '445' '136' '247' '118' '77' '76' '92' '198' '205' '299'\n",
      " '222' '245' '104' '153' '349' '114' '320' '292' '226' '420' '500' '325'\n",
      " '307' '78' '265' '108' '123' '189' '32' '58' '86' '219' '800' '335' '63'\n",
      " '229' '425' '67' '87' '1200' '158' '650' '234' '310' '695' '400' '166'\n",
      " '119' '62' '168' '340' '479' '43' '395' '144' '52' '47' '529' '187' '209'\n",
      " '233' '82' '269' '163' '172' '305' '156' '550' '435' '137' '124' '48'\n",
      " '279' '330' '5000' '134' '378' '97' '277' '64' '193' '147' '186' '264'\n",
      " '30' '3000' '112' '94' '379' '57' '415' '236' '410' '214' '88' '66' '71'\n",
      " '171' '157' '545' '1500' '83' '96' '1800' '81' '188' '380' '255' '505'\n",
      " '54' '33' '174' '93' '740' '640' '1300' '440' '599' '357' '1239' '495'\n",
      " '127' '5999' '178' '348' '152' '242' '183' '253' '750' '259' '365' '273'\n",
      " '197' '397' '103' '389' '355' '559' '38' '203' '999' '141' '162' '333'\n",
      " '698' '46' '360' '895' '10' '41' '206' '281' '449' '388' '212' '102'\n",
      " '201' '2750' '4750' '432' '675' '167' '390' '298' '339' '194' '302' '211'\n",
      " '595' '191' '53' '361' '480' '8000' '4500' '459' '997' '345' '216' '218'\n",
      " '111' '735' '276' '91' '490' '850' '398' '36' '775' '267' '625' '336'\n",
      " '2500' '176' '725' '3750' '469' '106' '460' '287' '575' '227' '263' '25'\n",
      " '228' '208' '177' '880' '148' '116' '685' '470' '217' '164' '61' '645'\n",
      " '699' '405' '252' '319' '268' '419' '343' '525' '311' '840' '154' '294'\n",
      " '950' '409' '184' '257' '204' '241' '2000' '412' '121' '288' '196' '900'\n",
      " '647' '524' '1750' '309' '510' '1495' '1700' '799' '383' '372' '492'\n",
      " '327' '1999' '656' '224' '173' '875' '1170' '795' '690' '146' '465'\n",
      " '1100' '151' '274' '429' '825' '282' '256' '1111' '620' '271' '161' '51'\n",
      " '855' '579' '1174' '430' '20' '899' '649' '485' '181' '455' '4000' '243'\n",
      " '342' '590' '560' '374' '437' '232' '359' '985' '31' '244' '254' '723'\n",
      " '237' '428' '370' '34' '1400' '580' '2520' '221' '749' '1600' '2695'\n",
      " '306' '202' '680' '570' '520' '223' '2295' '213' '1065' '346' '24' '286'\n",
      " '296' '266' '26' '995' '1368' '393' '182' '635' '258' '780' '589' '347'\n",
      " '1250' '1350' '446' '3200' '1050' '1650' '1550' '975' '323' '6500' '2499'\n",
      " '1850' '2250' '715' '461' '540' '356' '439' '384' '569' '1900' '22' '785'\n",
      " '626' '830' '318' '444' '321' '401' '1499' '888' '369' '770' '386' '366'\n",
      " '344' '630' '313' '597' '262' '509' '10000' '278' '312' '789' '1195'\n",
      " '422' '21' '765' '3500' '945' '326' '3100' '2486' '3390' '1356' '2599'\n",
      " '472' '454' '328' '396' '291'] \n",
      "\n",
      "[  145    37    28   199   549   149   250    90   270   290   170    59\n",
      "    49    68   285    75   100   150   700   125   175    40    89    95\n",
      "    99   499   120    79   110   180   143   230   350   135    85    60\n",
      "    70    55    44   200   165   115    74    84   129    50   185    80\n",
      "   190   140    45    65   225   600   109  1990    73   240    72   105\n",
      "   155   160    42   132   117   295   280   159   107    69   239   220\n",
      "   399   130   375   585   275   139   260    35   133   300   289   179\n",
      "    98   195    29    27    39   249   192   142   169  1000   131   138\n",
      "   113   122   329   101   475   238   272   308   126   235   315   248\n",
      "   128    56   207   450   215   210   385   445   136   247   118    77\n",
      "    76    92   198   205   299   222   245   104   153   349   114   320\n",
      "   292   226   420   500   325   307    78   265   108   123   189    32\n",
      "    58    86   219   800   335    63   229   425    67    87  1200   158\n",
      "   650   234   310   695   400   166   119    62   168   340   479    43\n",
      "   395   144    52    47   529   187   209   233    82   269   163   172\n",
      "   305   156   550   435   137   124    48   279   330  5000   134   378\n",
      "    97   277    64   193   147   186   264    30  3000   112    94   379\n",
      "    57   415   236   410   214    88    66    71   171   157   545  1500\n",
      "    83    96  1800    81   188   380   255   505    54    33   174    93\n",
      "   740   640  1300   440   599   357  1239   495   127  5999   178   348\n",
      "   152   242   183   253   750   259   365   273   197   397   103   389\n",
      "   355   559    38   203   999   141   162   333   698    46   360   895\n",
      "    10    41   206   281   449   388   212   102   201  2750  4750   432\n",
      "   675   167   390   298   339   194   302   211   595   191    53   361\n",
      "   480  8000  4500   459   997   345   216   218   111   735   276    91\n",
      "   490   850   398    36   775   267   625   336  2500   176   725  3750\n",
      "   469   106   460   287   575   227   263    25   228   208   177   880\n",
      "   148   116   685   470   217   164    61   645   699   405   252   319\n",
      "   268   419   343   525   311   840   154   294   950   409   184   257\n",
      "   204   241  2000   412   121   288   196   900   647   524  1750   309\n",
      "   510  1495  1700   799   383   372   492   327  1999   656   224   173\n",
      "   875  1170   795   690   146   465  1100   151   274   429   825   282\n",
      "   256  1111   620   271   161    51   855   579  1174   430    20   899\n",
      "   649   485   181   455  4000   243   342   590   560   374   437   232\n",
      "   359   985    31   244   254   723   237   428   370    34  1400   580\n",
      "  2520   221   749  1600  2695   306   202   680   570   520   223  2295\n",
      "   213  1065   346    24   286   296   266    26   995  1368   393   182\n",
      "   635   258   780   589   347  1250  1350   446  3200  1050  1650  1550\n",
      "   975   323  6500  2499  1850  2250   715   461   540   356   439   384\n",
      "   569  1900    22   785   626   830   318   444   321   401  1499   888\n",
      "   369   770   386   366   344   630   313   597   262   509 10000   278\n",
      "   312   789  1195   422    21   765  3500   945   326  3100  2486  3390\n",
      "  1356  2599   472   454   328   396   291] \n",
      "\n",
      "Total missing:  0\n"
     ]
    }
   ],
   "source": [
    "price = df['Price']\n",
    "price = price.str.replace(',','') # Replace commas with nothing\n",
    "print( price.unique() , '\\n')\n",
    "price = pd.to_numeric(price,errors='coerce') # Typecast price to float/numeric\n",
    "print( price.unique() , '\\n')\n",
    "print( 'Total missing: ', sum( price.isnull() ) ) # This converts all the values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47720c07-8f8f-4d01-8a5c-725821c52ce2",
   "metadata": {},
   "source": [
    "2. Categorical variable: For the `./data/sharks.csv` data covered in the lecture, clean the \"Type\" variable as well as you can, and explain the choices you make."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5ec6168-5179-4eaf-982a-0488e0e98956",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/sharks.csv', low_memory=False)\n",
    "# df.head()\n",
    "# df.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78c7cc9f-ea6c-49a5-a3de-0827964ab4bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Type\n",
       "Unprovoked             4716\n",
       "Provoked                593\n",
       "Invalid                 552\n",
       "Sea Disaster            239\n",
       "Watercraft              142\n",
       "Boat                    109\n",
       "Boating                  92\n",
       "Questionable             10\n",
       "Unconfirmed               1\n",
       "Unverified                1\n",
       "Under investigation       1\n",
       "Boatomg                   1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "baf58a1d-e509-44f9-bf87-4568441be346",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Type\n",
       "Unprovoked    4716\n",
       "Provoked       593\n",
       "Watercraft     583\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type = df['Type'] # Create a temporary vector of values for the Type variable to play with\n",
    "\n",
    "type = type.replace(['Sea Disaster', 'Boat','Boating','Boatomg'],'Watercraft') # All watercraft/boating values\n",
    "type.value_counts()\n",
    "\n",
    "type = type.replace(['Invalid', 'Questionable','Unconfirmed','Unverified','Under investigation'],np.nan) # All unclean values\n",
    "type.value_counts()\n",
    "\n",
    "df['Type'] = type # Replace the 'Type' variable with the cleaned version\n",
    "del type # Destroy the temporary vector\n",
    "\n",
    "df['Type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b7a980c-0db2-4202-a077-1b215d6af110",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Fatal (Y/N)</th>\n",
       "      <th>N</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Type</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Provoked</th>\n",
       "      <td>0.967521</td>\n",
       "      <td>0.032479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Unprovoked</th>\n",
       "      <td>0.743871</td>\n",
       "      <td>0.256129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Watercraft</th>\n",
       "      <td>0.684303</td>\n",
       "      <td>0.315697</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Fatal (Y/N)         N         Y\n",
       "Type                           \n",
       "Provoked     0.967521  0.032479\n",
       "Unprovoked   0.743871  0.256129\n",
       "Watercraft   0.684303  0.315697"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is interesting: Sharks are much more likely to kill you if the situation is UNprovoked. \n",
    "df['Fatal (Y/N)'] = df['Fatal (Y/N)'].replace(['UNKNOWN', 'F','M','2017'],np.nan) # All unclean values\n",
    "df['Fatal (Y/N)'] = df['Fatal (Y/N)'].replace('y','Y') # All unclean values\n",
    "pd.crosstab(df['Type'],df['Fatal (Y/N)'],normalize='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf6dd4f-6f57-4d2f-be4c-253ee9fbb018",
   "metadata": {},
   "source": [
    "3. Dummy variable: For the pretrial data covered in the lecture, clean the `WhetherDefendantWasReleasedPretrial` variable as well as you can, and, in particular, replace missing values with `np.nan`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60928a9b-2daf-4940-9b8a-77367000b0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/VirginiaPretrialData2017.csv', low_memory=False)\n",
    "# df.head()\n",
    "# df.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cbb0010a-73f4-4e21-a4cf-f643f4579d64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9 0 1] \n",
      "\n",
      "WhetherDefendantWasReleasedPretrial\n",
      "1    19154\n",
      "0     3801\n",
      "9       31\n",
      "Name: count, dtype: int64 \n",
      "\n",
      "WhetherDefendantWasReleasedPretrial\n",
      "1.0    19154\n",
      "0.0     3801\n",
      "Name: count, dtype: int64 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "release = df['WhetherDefendantWasReleasedPretrial']\n",
    "print(release.unique(),'\\n')\n",
    "print(release.value_counts(),'\\n')\n",
    "release = release.replace(9,np.nan) # In the codebook, the 9's are \"unclear\"\n",
    "print(release.value_counts(),'\\n')\n",
    "sum(release.isnull()) # 31 missing values\n",
    "df['WhetherDefendantWasReleasedPretrial'] = release # Replace data column with cleaned values\n",
    "del release"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e983b47-c6fe-4ca3-ab7d-caaab37a7017",
   "metadata": {},
   "source": [
    "4. Missing values, not at random: For the pretrial data covered in the lecture, clean the `ImposedSentenceAllChargeInContactEvent` variable as well as you can, and explain the choices you make. (Hint: Look at the `SentenceTypeAllChargesAtConvictionInContactEvent` variable.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "68692d9e-b8ff-415d-b0a7-ce71cfce4bfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9053 \n",
      "\n",
      "SentenceTypeAllChargesAtConvictionInContactEvent     0     1    2     4    9\n",
      "ImposedSentenceAllChargeInContactEvent                                      \n",
      "False                                             8720  4299  914     0    0\n",
      "True                                                 0     0    0  8779  274 \n",
      "\n",
      "SentenceTypeAllChargesAtConvictionInContactEvent     0     1    2     4    9\n",
      "ImposedSentenceAllChargeInContactEvent                                      \n",
      "False                                             8720  4299  914  8779    0\n",
      "True                                                 0     0    0     0  274 \n",
      "\n",
      "274 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "length = df['ImposedSentenceAllChargeInContactEvent']\n",
    "type = df['SentenceTypeAllChargesAtConvictionInContactEvent']\n",
    "\n",
    "# print( length.unique()  , '\\n') # Some values are ' ', denoting missing\n",
    "length = pd.to_numeric(length,errors='coerce') # Coerce to numeric\n",
    "length_NA = length.isnull() # Create a missing dummy\n",
    "print( np.sum(length_NA),'\\n') # 9k missing values of 23k, not so good\n",
    "\n",
    "print( pd.crosstab(length_NA, type), '\\n') # Category 4 is cases where the charges were dismissed\n",
    "\n",
    "length = length.mask( type == 4, 0) # Replace length with 0 when type ==4\n",
    "length = length.mask( type == 9, np.nan) # Replace length with np.nan when type == 9\n",
    "\n",
    "length_NA = length.isnull() # Create a new missing dummy\n",
    "print( pd.crosstab(length_NA, type), '\\n')\n",
    "print( np.sum(length_NA),'\\n') # 274 missing, much better\n",
    "\n",
    "df['ImposedSentenceAllChargeInContactEvent'] = length # Replace data with cleaned version\n",
    "del length, type # Delete temporary length/type variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3968e4-5084-4f28-b6ba-b8932eb77e98",
   "metadata": {},
   "source": [
    "**Q3.** \n",
    "\n",
    "Many important datasets contain a race variable, typically limited to a handful of values often including Black, White, Asian, Latino, and Indigenous. Likewise, many important datasets contain a sex or gender variable, typically limited to a handful of values often including Male and Female. \n",
    "\n",
    "1. How did the most recent US Census gather data on sex, gender, sexuality, and race?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633754bc-9db7-4db2-940a-caf973cabbf4",
   "metadata": {},
   "source": [
    "I looked at the 2020 Census, the 2020 American Community Survey (ACS), and the 2020 Household Pulse Survey (HPS), to compare and contrast how the Census Bureau is approaching these questions. People typically mean the ACS when they talk about Census data\n",
    "\n",
    "- For the 8-page Census and 20-page 2020 ACS, the questions are very similar:\n",
    "    -  Sex is binary Male/Female, but doesn't distinguish between assignment at birth versus current identification\n",
    "    -  Race is collected as a top level \"check all that apply\", with write-in boxes for more nuance (e.g. White, then English). For ethnicity, Hispanic/Latino/Spanish origin is collected, along with more specific details about origin (e.g. Cuban, Mexican, Puerto Rican)\n",
    "    -  No SOGI (Sexual Orientation and Gender Identity) questions\n",
    " \n",
    "- For the 44-page 2020 HPS,\n",
    "    -  Race is similar to ACS/Census (check all that apply) but the questions aren't broken out by broad categories (white, Black, indigenous, etc.)\n",
    "    -  For sex, \"What sex were you assigned at birth, on your original birth certificate?\", along with \"Do you currently describe yourself as male, female or transgender?\" with a \"None of these\" option. There's an alternative version of the question that includes \"Non-binary\"\n",
    "    -  For SOGI, there is a question, \"Which of the following best represents how you think of yourself?\" including \"Gay or lesbian\",  \"Straight, that is not gay or lesbian\", \"Bisexual\", \"Something else\", \"I don't know\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa5647e-93fb-4aea-a9cc-ca393d0801c3",
   "metadata": {},
   "source": [
    "3. Why do we gather these data? What role do these kinds of data play in politics and society? Why does data quality matter?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b582c0-1670-4119-b460-ba40190cd5d9",
   "metadata": {},
   "source": [
    "> There are many reasons to do this and the data play many roles. One would be economic: They determine how need is measured, how aid is distributed, and what kinds of policies are proposed and enacted. One would be sociological: Knowing the true proportions of the populations that fall into each demographic group is important for understanding who we are, and can foster tolerance and inclusion. Particular areas of the country might be homogeneous, but being able to describe how diverse the country really is in objective terms can be important in crafting policy and helping people understand how complex the needs and identities of other citizens are. If the data don't capture that information or do so in a "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b195770-c94b-44cd-a60b-0789592b7241",
   "metadata": {},
   "source": [
    "4. Please provide a constructive criticism of how the Census was conducted: What was done well? What do you think was missing? How should future large scale surveys be adjusted to best reflect the diversity of the population? Could some of the Census' good practices be adopted more widely to gather richer and more useful data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d4111b-7349-4457-9472-52614226a77a",
   "metadata": {},
   "source": [
    "> For race, at least, I think they do a reasonable job of giving people the identify with one or more broad categories, but then also provide additional nuance. Older versions of the census tried to ask questions like, \"Which group do you most identify as?\" but it's unclear whether that was effective in gathering information, and researchers probably just used that variable and ignored the nuance of the rest of the information gathered.\n",
    "\n",
    "> For SOGI, the HPS question is probably better overall in 2023. The question about \"assigned at birth\" might be somewhat intrusive and doesn't give options for the respondent to say \"I don't know\" or \"I prefer not to answer\", which is bad practice because it forces the enumerator to enter something. The question then follows up with a more nuanced set of categories that address contemporary discussions about sex and gender. The HPS version isn't perfect, but the ACS/Census version provides no nuance or measurement at all. Likewise, ACS/Census gathers no data on sexuality, but HPS makes an attempt to get information on LGB and S identification.\n",
    "\n",
    "> I've seen a lot of race questions that only allow one identification and only offer categories of \"White, Black, Asian, Other\" that really lack nuance, so this \"check all that apply\" approach with some broad categories as well as supplementary information seems like a major improvement. Likewise, most surveys have limited options for SOGI data or none, so more nuanced data gathering like the HPS (which isn't perfect, but makes a sincere attempt) will be important in the future; for example, some surveys suggest 16-20% of people under 25 identify as LGBT, even if that number has historically been between 5% to 7%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194a3b6c-e029-4fe9-99c8-70a7c60ce7a7",
   "metadata": {},
   "source": [
    "5. When it comes to cleaning and analyzing data, what concerns do you have about potentially sensitive variables like sex, gender, sexual identity, or race? What challenges can you imagine arising when there are missing values? What good or bad practices might people adopt, and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83bc3dc3-283a-40db-bb9a-5abf2c2c4936",
   "metadata": {},
   "source": [
    "> I think the biggest problem is probably that we force people into making a \"pie chart\" of who they are without making the meaning of the exercise very explicit. Let's take race, for example. \"What is your race?\" might mean something like, \"Roughly, what percentage of your great grandparents were members of the following groups: White, Black, Asian, Indigenous?\" For example, I am 7/8 Irish and 1/8 German. That is one way to make sense of the question, but seems tied to a historical idea of Americans as immigrants with a specific mix of origination countries. Today, that isn't really how a lot of people think about themselves or their culture, and those great grandparents will themselves have complex, multi-dimensional identities. A person can fully be a member of multiple racial communities, and the appropriate way to let them express that kind of identity is to ask, \"Of the following groups, which do you consider yourself a member?\", close to what the Census is doing. Similarly, when it comes to SOGI questions, it's likewise better to let people pick affiliation or not with a variety of categories, rather than construct a question that forces them into a one-dimensional answer.\n",
    "\n",
    "> To be a little more formal about this, imagine it's a choice between category A and category B. We could model it as a dummy variable, `D`, with `D=0` meaning group A and `D=1` meaning group B. That means you have to pick A or B, but not both and not neither. Likewise, a \"check at most one\" question with many categories will lead to a categorical variable where you have `A` or `B` or nothing. A \"check all that apply\" question means you have a vector of categories and any combination of ones and zeros is possible, like with race on the Census; this would allow answers of `AB`, `A`, `B`, and nothing. You might want to go even further in some cases, and gather a numeric value that expresses the intensity of association with each category, like $(A=.75, B=.8)$, allowing values to sum to greater than 1, but I doubt any surveys actually do this because it would be hard to elicit these numbers in a consistent and meaningful way.\n",
    "\n",
    "> When there are missing values in race/sex/gender categories, people will be tempted to impute values. However, the people who don't answer these questions probably have a reason for doing so, and using the available to impute values is not responsible because these people are already systematically different from those who answered. For example, if someone has complex ideas about their identity and they opt out of answering, turning around and \"filling in the blanks\" for them based on other demographic data is worse than wrong, because it undermines their agency in deciding not to answer. These kinds of complications have to be considered when you're dealing with self-reported data from people. \n",
    "\n",
    "> My concern when people clean identity-type data is that they'll compress things down to the \"main\" answers and remove all nuance from the discussion (e.g. aggregate all of the identities from Asian [Chinese, Indian, Japanese, Korean, Vietnamese, and so on] into a single bucket). So the survey goes to a lot of work to gather lots of nuanced data, but then all the nuance might end up erased when it's cleaned to have simple categories for modeling."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
